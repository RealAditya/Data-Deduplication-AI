# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uDFAD1dYZVH2F7KK5lOOLC5xGTA_WdwU
"""

!pip install fuzzywuzzy

import pandas as pd
from fuzzywuzzy import fuzz
import numpy as np
import re

# Preprocessing function
def preprocess_data(df):
    # Convert all text to lowercase
    df = df.applymap(lambda s: s.lower() if type(s) == str else s)

    # Strip leading/trailing whitespace
    df = df.applymap(lambda s: s.strip() if type(s) == str else s)

    # Fill missing values with a placeholder (e.g., empty string)
    df = df.fillna('')

    return df

# Function to calculate similarity for numeric values
def numeric_similarity(value1, value2, tolerance):
    return abs(value1 - value2) <= tolerance

# Function to identify potential duplicates within a block using fuzzy matching
def find_duplicates_block(block, threshold, numeric_tolerance):
    duplicates = []
    num_columns = len(block.columns)
    for i, row1 in block.iterrows():
        for j, row2 in block.iterrows():
            if i < j:  # Ensure each pair is only checked once
                similarities = []
                for col in block.columns:
                    if block[col].dtype == 'object':
                        similarity = fuzz.token_sort_ratio(str(row1[col]), str(row2[col]))
                    else:
                        similarity = 100 if numeric_similarity(row1[col], row2[col], numeric_tolerance) else 0
                    similarities.append(similarity)

                # Calculate average similarity across all fields
                average_similarity = sum(similarities) / num_columns

                if average_similarity >= threshold:
                    duplicates.append(j)  # Only need to drop one of the duplicates
    return list(set(duplicates))

# Function to remove duplicates from DataFrame
def remove_duplicates(df, duplicate_indices):
    df.drop(duplicate_indices, inplace=True)

# Blocking function to group similar records together
def blocking(df):
    # Select columns with the highest cardinality (most unique values) for blocking
    cardinalities = df.nunique().sort_values(ascending=False)
    block_columns = cardinalities.index[:2]  # Select the top 2 columns with highest cardinality

    df['block_key'] = df[block_columns].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)
    return df

# Parameters
chunk_size = 10000
threshold = 99
numeric_tolerance = 5  # Tolerance for numeric similarity

# Initialize an empty DataFrame for the cleaned data
cleaned_data = pd.DataFrame()

# Read the CSV file in chunks
for chunk in pd.read_csv('smtgo.csv', chunksize=chunk_size):
    # Preprocess the data in the chunk
    chunk = preprocess_data(chunk)

    # Apply blocking to the chunk
    chunk = blocking(chunk)

    # Find potential duplicates within each block
    for block_key, block in chunk.groupby('block_key'):
        potential_duplicates = find_duplicates_block(block, threshold, numeric_tolerance)
        remove_duplicates(block, potential_duplicates)

        # Append the cleaned block to the cleaned_data DataFrame
        cleaned_data = pd.concat([cleaned_data, block.drop(columns='block_key')])

# Write the cleaned data to a new CSV file
cleaned_data.to_csv('cleaned_file.csv', index=False)